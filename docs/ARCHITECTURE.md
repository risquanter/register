# Risk Register - Architecture Documentation

**Last Updated:** February 9, 2026  
**Status:** Active Development  
**Version:** 0.1.0

---

## ðŸ“‹ Table of Contents

1. [Executive Summary](#executive-summary)
2. [Current Architecture State](#current-architecture-state)
3. [Architectural Strengths](#architectural-strengths)
4. [Known Architectural Gaps](#known-architectural-gaps)
5. [Technology Stack](#technology-stack)
6. [Domain Model](#domain-model)
7. [Layered Architecture](#layered-architecture)
8. [Data Flow](#data-flow)
9. [Validation Strategy](#validation-strategy)
10. [Parallel Execution Model](#parallel-execution-model)
11. [Testing Strategy](#testing-strategy)
12. [Future Roadmap](#future-roadmap)
13. [Deployment Architecture](#deployment-architecture)
14. [Appendix A: HDR Histogram for Million-Scale Trials](#appendix-a-hdr-histogram-for-million-scale-trials)
15. [Appendix B: ZIO Metrics Bridge for Runtime Observability](#appendix-b-zio-metrics-bridge-for-runtime-observability)

---

## Executive Summary

**Risk Register** is a functional Scala application for hierarchical Monte Carlo risk simulation. It enables users to model complex risk portfolios as trees, execute simulations, and analyze Loss Exceedance Curves (LECs).

**Current State:**
- âœ… 512 tests passing (289 common + 223 server)
- âœ… Core simulation engine functional with parallel execution
- âœ… Type-safe domain model using Iron refinement types
- âœ… Clean separation of concerns (domain/service/HTTP layers)
- âœ… Typed error codes with field path context (ValidationErrorCode)
- âœ… BuildInfo integration for version management
- âœ… Sparse storage architecture validated and optimized
- âœ… Configuration externalized via ZIO Config + application.conf with env-var overrides
- âœ… Structured logging with ZIO + Logback (JSON encoder pending for production)
- âœ… DTO/Domain separation complete (validation-during-parsing)

---

## Current Architecture State

### âœ… **What Works Well**

#### 1. **Type Safety with Iron Refinement Types**
```scala
// Compile-time validation with zero runtime overhead
opaque type SafeIdStr = String :| Match["^[0-9A-HJKMNP-TV-Z]{26}$"]  // ULID (Crockford base32)
type Probability = Double :| (Greater[0.0] & Less[1.0])
```

**Benefits:**
- Impossible to pass invalid data to domain models
- Errors caught at compile time
- Self-documenting code (types encode constraints)

#### 2. **Error Accumulation with ZIO Prelude**
```scala
// Parallel validation - collects ALL errors, not just first failure
val validated = idValidation
  .zipPar(nameValidation)
  .zipPar(probabilityValidation)
```

**Benefits:**
- Better UX (users see all errors at once)
- Functional approach (no exceptions)
- Composable validation logic

#### 3. **Flat ADT for Risk Trees**
```scala
sealed trait RiskNode
case class RiskLeaf(..., parentId: Option[NodeId]) extends RiskNode
case class RiskPortfolio(..., childIds: Array[NodeId], parentId: Option[NodeId]) extends RiskNode
```

**Benefits:**
- Flat node collection with ID references (not recursive nesting)
- Type-safe pattern matching with compiler-verified exhaustive matching
- Enables O(1) node lookup via `TreeIndex`

#### 4. **Parallel Execution**
```scala
// ZIO structured concurrency for tree traversal
ZIO.collectAllPar(children.map(simulate)).withParallelism(8)

// Scala parallel collections for CPU-bound trials
successfulTrials.par.map(computeLoss).toVector
```

**Benefits:**
- Efficient utilization of multi-core CPUs
- Configurable parallelism
- Deterministic results (same seed = same output)

#### 5. **Clean Layer Dependency Injection**
```scala
val appLayer = ZLayer.make[RiskTreeController & Server](
  Server.default,
  RiskTreeRepositoryInMemory.layer,
  RiskResultResolverLive.layer,
  TreeCacheManager.layer,
  RiskTreeServiceLive.layer,
  ZLayer.fromZIO(RiskTreeController.makeZIO)
)
```

**Benefits:**
- Type-safe dependency resolution
- No manual wiring
- Testable (can swap implementations)

---

## Architectural Considerations

### **DTO/Domain Model Separation** (Implemented)

**Status:** âœ… Complete (see IMPLEMENTATION_PLAN.md Phase 2)

**Approach:** Validation-during-parsing with private intermediate DTOs

**Key Design Points:**
- Request DTOs use plain types (String, Double, Long)
- Custom JSON decoders validate via smart constructors during deserialization  
- Private domain constructors enforce validation
- Response DTOs use `fromDomain()` factories
- Error accumulation collects all validation errors in one pass
- Field paths use ID-based format (e.g., `"riskLeaf[id=cyber].probability"`)

**Benefits over alternatives:**
- API stability: Domain model changes don't break HTTP contracts
- Validation guarantee: Impossible to bypass smart constructors
- Better UX: Users see all errors at once, not fail-fast
- Testability: Can test HTTP, validation, and business logic independently

---

### **Structured Logging** (Implemented)

**Status:** âœ… Application-level logging complete

**Current:** ZIO logging (`ZIO.logInfo`, `logWarning`, `logDebug`, `logError`) used throughout all service layers per ADR-002. Routed via `zio-logging-slf4j2` bridge to Logback with env-configurable levels (`LOG_LEVEL`). OpenTelemetry tracing and metrics fully integrated via `TelemetryLive` (console + OTLP exporters).

**Remaining (production deployment):** Swap Logback plain-text encoder to JSON encoder (e.g., `logstash-logback-encoder`) for structured log aggregation in containerised environments. Request-ID correlation and user context headers are Kubernetes deployment concerns, not application-level gaps.

---

### **User Context Extraction** (Future)

**Status:** Planned for Kubernetes deployment

**Strategy:** Trust service mesh for authentication, extract user context from injected headers (`X-User-Id`, `X-User-Roles`), propagate via ZIO FiberRef for audit logging.

---

### **Rate Limiting** (Future)

**Status:** Delegated to infrastructure (Kubernetes Ingress + Service Mesh)

**Strategy:** Multi-layer approach with infrastructure handling network/service-level limits, application handling business logic limits (e.g., concurrent LEC computations per user).

---

## Technology Stack

### **Core Framework**
- **Scala:** 3.6.4
- **ZIO:** 2.1.24 (effect system, concurrency, dependency injection)
- **ZIO Prelude:** 1.0.0-RC44 (Validation, type classes)
- **ZIO Test:** 2.1.24 (property-based testing)

### **Type Safety**
- **Iron:** 3.2.2 (refinement types, opaque types)
- **Metalog Distribution:** Custom implementation

### **HTTP Layer**
- **Tapir:** 1.13.4 (endpoint definition)
- **ZIO HTTP:** (server implementation)
- **ZIO JSON:** 0.8.0 (serialization)

### **Configuration**
- **ZIO Config:** 4.0.2 (with zio-config-magnolia, zio-config-typesafe)
- **Typesafe Config:** 1.4.3

### **Logging**
- **ZIO Logging:** 2.5.2
- **Logback:** 1.5.23
- **ZIO Logging SLF4J Bridge:** 2.5.2

### **Observability**
- **ZIO Telemetry:** 3.1.13 (OpenTelemetry bridge for tracing + metrics)
- **OpenTelemetry SDK:** 1.57.0 (traces, metrics, OTLP + console exporters)

### **Testing**
- **ZIO Test:** Unit & property-based tests
- **ScalaCheck:** Property generators (via Iron)
- **Testcontainers:** (Planned - for integration tests)

### **Build Tools**
- **SBT:** 1.x
- **ScalaJS:** Cross-compilation for frontend

---

## Domain Model

### **Risk Tree Structure**

```
RiskNode (sealed trait)
â”œâ”€â”€ RiskLeaf (terminal node - actual risk)
â”‚   â”œâ”€â”€ id: NodeId                              // ULID-based nominal wrapper
â”‚   â”œâ”€â”€ name: SafeName
â”‚   â”œâ”€â”€ distributionType: "expert" | "lognormal"
â”‚   â”œâ”€â”€ probability: Probability (0.0 < p < 1.0)
â”‚   â”œâ”€â”€ percentiles: Option[Array[Double]]      // Expert mode
â”‚   â”œâ”€â”€ quantiles: Option[Array[Double]]         // Expert mode
â”‚   â”œâ”€â”€ minLoss: Option[NonNegativeLong]         // Lognormal mode
â”‚   â”œâ”€â”€ maxLoss: Option[NonNegativeLong]         // Lognormal mode
â”‚   â””â”€â”€ parentId: Option[NodeId]                 // None for root
â”‚
â””â”€â”€ RiskPortfolio (branch node - aggregation)
    â”œâ”€â”€ id: NodeId
    â”œâ”€â”€ name: SafeName
    â”œâ”€â”€ childIds: Array[NodeId]                  // ID references (flat)
    â””â”€â”€ parentId: Option[NodeId]                 // None for root
```

### **Domain Entities**

```scala
// Configuration (persisted, no simulation results)
final case class RiskTree(
  id: TreeId,                          // ULID-based nominal wrapper
  name: SafeName.SafeName,
  nodes: Seq[RiskNode],                // Flat collection (not nested root)
  rootId: NodeId,                      // Root node reference
  index: TreeIndex                     // O(1) node lookup by NodeId
)

// Node identity uses nominal wrapper (ADR-018)
case class NodeId(toSafeId: SafeId.SafeId)
case class TreeId(toSafeId: SafeId.SafeId)

// Simulation results are per-node, cached in-memory
final case class RiskResult(
  nodeId: NodeId,
  outcomes: Map[TrialId, Loss],
  provenances: List[NodeProvenance]
)

// Metalog Distribution (compact representation)
final case class LossDistribution(
  coefficients: Array[Double],         // ~10 values instead of 10K samples
  lowerBound: Option[Double],
  upperBound: Option[Double]
)
```

**Key Design Decision: No Raw Trial Data Stored**
- âœ… Store: Metalog coefficients (~100 bytes)
- âŒ Don't store: Raw trial arrays (~80KB for 10K trials)
- **Benefit:** Compact storage, reproducible results, fast deserialization

### **Sparse Storage: Current Implementation Benefits**

The current sparse storage approach using `Map[TrialId, Loss]` provides excellent scaling characteristics:

- âœ… **Handles 10k trials efficiently today** - Sparse maps with 100-5000 entries (typical for low-probability events)
- âœ… **Scales to 1M trials with more cores/memory** - Parallel collections leverage multi-core CPUs without architectural changes
- âœ… **Scales to 100M trials with distributed workers** - Referential transparency enables trivial distribution across compute nodes
- âœ… **Preserves exact aggregation semantics at any scale** - Trial-level merge maintains mathematical correctness for portfolio aggregation
- âœ… **Maintains provenance/reproducibility guarantees** - Complete trial-by-trial reconstruction from stored metadata

**Why sparse storage works well:**
- **Memory efficiency for low-probability events:** A 0.01 probability risk with 10,000 trials stores ~100 map entries (~1.6KB), not 10,000 entries
- **Parallel-friendly:** Pure functions enable lock-free parallelization at both trial and risk levels
- **Identity-based aggregation:** Associative merge operation scales to arbitrary tree depths without recomputation
- **Deterministic parallelism:** HDR-based PRNG ensures identical results regardless of parallelization strategy

See [Appendix A: HDR Histogram for Million-Scale Trials](#appendix-a-hdr-histogram-for-million-scale-trials) for detailed comparison with alternative approaches.

---

## Layered Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HTTP Layer (modules/server)                     â”‚
â”‚  - Controllers: Request/response handling       â”‚
â”‚  - Endpoints: Tapir route definitions           â”‚
â”‚  - Middleware: CORS, error handling             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service Layer (modules/server)                  â”‚
â”‚  - RiskTreeService: Business logic              â”‚
â”‚  - RiskResultResolver: Simulation orchestration â”‚
â”‚  - TreeCacheManager: Result caching             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Domain Layer (modules/common)                   â”‚
â”‚  - RiskNode: ADT for risk trees                 â”‚
â”‚  - RiskTree: Aggregate root                     â”‚
â”‚  - Smart Constructors: Validation logic         â”‚
â”‚  - Iron Types: Refinement types                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Simulation Engine (modules/server)              â”‚
â”‚  - Simulator: Monte Carlo execution             â”‚
â”‚  - RiskSampler: Distribution sampling           â”‚
â”‚  - Metalog: Distribution fitting                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Repository Layer (modules/server)               â”‚
â”‚  - RiskTreeRepository: Persistence interface    â”‚
â”‚  - InMemory: Development implementation         â”‚
â”‚  - (Future: PostgreSQL, DynamoDB)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Dependency Rules:**
- HTTP â†’ Service â†’ Domain âœ…
- Service â†’ Repository âœ…
- Domain â†’ Nothing (pure) âœ…
- HTTP âŒâ†’ Repository (bypass service)
- HTTP âŒâ†’ Simulation (bypass service)

---

## Data Flow

### **1. Create Risk Tree (Config Only)**
```
POST /risk-trees

JSON Request
  â†“
RiskTreeDefinitionRequest (DTO)
  â†“ [Validation]
RiskTree (Domain)
  â†“ [Repository]
Persisted RiskTree
  â†“ [DTO Conversion]
SimulationResponse (without LEC)
```

### **2. Compute LEC (On-Demand)**
```
GET /risk-trees/:treeId/nodes/:nodeId/lec

Load RiskTree from Repository
  â†“
RiskResultResolver.ensureCached(tree, nodeId)
  â”‚
  â”œâ”€â†’ Cache hit: Return cached RiskResult
  â”‚
  â””â”€â†’ Cache miss:
      â”œâ”€â†’ RiskLeaf: Create Metalog â†’ Sample trials â†’ RiskResult
      â””â”€â†’ RiskPortfolio: Simulate children in parallel â†’ Aggregate
  â†“
RiskResult (per-node, cached in TreeCacheManager)
  â†“
LECGenerator.generateCurvePoints
  â†“
LECCurveResponse (JSON)
```

**Key Insight:** Configuration (POST) and Computation (GET /lec) are separate.
- **POST:** Fast, synchronous, persists tree definition
- **GET /lec:** Computes per-node, caches results, incremental recomputation

---

## Validation Strategy

### **Layer 1: Iron Refinement Types (Compile-Time)**
```scala
type SafeId = String :| (Not[Blank] & MinLength[3] & MaxLength[30] & Match["^[a-zA-Z0-9_-]+$"])
```
- **When:** Compile time (if literal) or smart constructor call
- **What:** Primitive validation (length, format, range)
- **Error:** Compile error or `Validation[String, T]`

### **Layer 2: Smart Constructors (Business Rules)**
```scala
def create(
  id: String,
  distributionType: String,
  minLoss: Option[Long],
  maxLoss: Option[Long]
): Validation[String, RiskLeaf] = {
  // Cross-field validation
  if (distributionType == "lognormal" && minLoss.exists(_ >= maxLoss.getOrElse(0L)))
    Validation.fail("minLoss must be < maxLoss")
  else
    // ... refine and construct
}
```
- **When:** Domain object creation
- **What:** Business rules, cross-field validation
- **Error:** Accumulated via `Validation`

### **Layer 3: Service Layer (Orchestration)**
```scala
override def create(req: RiskTreeDefinitionRequest): Task[RiskTree] = {
  for {
    _ <- ZIO.fromEither(ValidationUtil.refinePositiveInt(req.nTrials, "nTrials"))
    safeName <- ZIO.fromEither(ValidationUtil.refineName(req.name))
    // req.root already validated by smart constructors during JSON deserialization
    persisted <- repo.create(riskTree)
  } yield persisted
}
```
- **When:** Request processing
- **What:** Coordinate validation, handle errors, orchestrate
- **Error:** `Task[A]` (ZIO effect type)

**Error Accumulation:**
```scala
// Parallel validation accumulates ALL errors
idValidation.zipPar(nameValidation).zipPar(probValidation)

// Result: Either success OR list of ALL validation failures
Left(Chunk("id too short", "name blank", "probability out of range"))
```

---

## Parallel Execution Model

### **Current Implementation: Hybrid Approach** âœ…

#### **Level 1: ZIO Structured Concurrency (Tree Traversal)**
```scala
// Ensure child results are cached (parallel)
ZIO.foreachPar(
  childIds.map(childId => resolver.ensureCached(tree, childId))
)
```

**Benefits:**
- **Structured:** Parent waits for all children
- **Interruptible:** Can cancel mid-execution
- **Error handling:** Failures don't crash thread pool
- **Configurable:** `withParallelism(n)` controls concurrency

#### **Level 2: Scala Parallel Collections (Trial Computation)**
```scala
// Compute losses in parallel (CPU-bound)
val successfulTrials = trials.filter(_.occurred).par
val losses = successfulTrials.map { trial => computeLoss(trial) }.toVector
```

**Benefits:**
- **Efficient:** Leverages work-stealing thread pool
- **Simple:** Familiar collection API
- **Deterministic:** Same seed = same results

### **Proposed Migration: Pure ZIO** (Optional Future Enhancement)

```scala
// Replace .par.map with ZIO.foreachPar
ZIO.foreachPar(successfulTrials) { trial =>
  ZIO.attempt(computeLoss(trial))
}.withParallelism(parallelism)
```

**Additional Benefits:**
- **Interruptible:** Can cancel long simulations
- **Telemetry:** Trace parallel execution spans
- **Error recovery:** Better failure handling

**Status:** Optional - current implementation provides excellent performance characteristics

---

## Testing Strategy

### **Current Test Coverage: 512 Tests**
- **Common Module:** 289 tests (domain model, validation, Iron types, tree operations)
- **Server Module:** 223 tests (service, simulation, HTTP, cache, SSE, provenance)
- **Focus:** Simulation determinism, parallel execution, tree aggregation, cache invalidation

### **Test Pyramid**

```
           /\
          /  \  E2E Tests (Future: K8s + Testcontainers)
         /â”€â”€â”€â”€\
        /      \  Integration Tests (Future: Redis, Postgres)
       /â”€â”€â”€â”€â”€â”€â”€â”€\
      /          \  Unit Tests (Current: 512 tests)
     /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
    /              \  Property Tests (Current: Algebraic laws)
   /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
```

### **Test Categories**

#### **1. Property-Based Tests**
```scala
test("Identity law: x + 0 = x") {
  check(Gen.long) { loss =>
    val zero = Identity[Loss].identity
    Identity[Loss].combine(loss, zero) == loss
  }
}

test("Associativity: (a + b) + c = a + (b + c)") { ... }
test("Commutativity: a + b = b + a") { ... }
```

#### **2. Smart Constructor Tests**
```scala
test("rejects id too short (< 3 chars)") {
  val result = RiskLeaf.create(id = "ab", ...)
  assertTrue(result.isFailure)
}

test("accumulates all field validation errors") {
  val result = RiskLeaf.create(
    id = "x",              // Too short
    name = "",             // Empty
    probability = 2.0      // Out of range
  )
  result.toEither match {
    case Left(errors) =>
      assertTrue(
        errors.length >= 3,
        errors.mkString("; ").contains("id"),
        errors.mkString("; ").contains("name"),
        errors.mkString("; ").contains("prob")
      )
  }
}
```

#### **3. Simulation Tests**
```scala
test("determinism: same seed produces identical results") {
  val run1 = Simulator.simulate(samplers, nTrials = 500, parallelism = 2)
  val run2 = Simulator.simulate(samplers, nTrials = 500, parallelism = 2)
  assertTrue(run1.map(_.outcomes) == run2.map(_.outcomes))
}

test("parallel vs sequential produce identical results") {
  val seq = Simulator.simulateSequential(samplers, nTrials = 800)
  val par = Simulator.simulate(samplers, nTrials = 800, parallelism = 8)
  assertTrue(seq.map(_.outcomes).toSet == par.map(_.outcomes).toSet)
}
```

#### **4. Integration Tests** (Planned)
```scala
// Testcontainers for Redis rate limiting
test("rate limiter persists across restarts") {
  withTestcontainers(redis, app) { ... }
}

// Testcontainers for PostgreSQL repository
test("repository persists risk trees") {
  withTestcontainers(postgres) { ... }
}
```

#### **5. Load Tests** (Future)
```scala
// K3s cluster + k6 load testing
test("handles 100 concurrent requests") {
  k6.run(scenario = "spike_test.js")
}
```

---

## Future Roadmap

### **Intended Use Cases (Not Yet Implemented)**

#### **1. Iterative Tree Building**
```
User Flow:
1. Create initial tree (POST /risk-trees)
2. Compute LEC (GET /risk-trees/:id/lec)
3. Modify tree (PUT /risk-trees/:id)
4. Re-compute LEC
5. Compare results
```

**Technical Needs:**
- âœ… Already have: CRUD for risk trees
- ðŸ”„ Future: Tree diff visualization
- ðŸ”„ Future: Incremental recomputation (only changed branches)

#### **2. Visual Tree Navigation**
```
Frontend Features:
- Focus on a node (zoom in)
- Expand/collapse portfolio levels
- Drill down to leaf details
- Show LEC at each level
```

**Technical Needs:**
- âœ… Already have: Per-node LEC via `LECCurveResponse` (flat `childIds` structure)
- ðŸ”„ Future: Frontend implementation (Scala.js + Laminar + Vega-Lite)
- ðŸ”„ Future: Memoization/caching of expanded nodes

#### **3. Scenario Analysis**
```
Workflow:
1. Baseline: Original risk tree
2. Scenario 1: Increase probability of cyber attack
3. Scenario 2: Add mitigation (reduces maxLoss)
4. Compare: Baseline vs Scenario 1 vs Scenario 2
```

**Technical Needs:**
- âœ… Already have: Can create multiple trees
- ðŸ”„ Future: Scenario branching (fork from baseline)
- ðŸ”„ Future: Visual comparison (side-by-side LECs)
- ðŸ”„ Future: Delta calculation (quantile differences)

#### **4. Metalog-Based Reproducibility**
```
Persistence Strategy:
- Store: Metalog coefficients (~10 doubles)
- Don't store: Raw trial data (~10K doubles)
- Benefit: Compact, reproducible, fast
```

**Technical Needs:**
- âœ… Already have: Metalog distribution fitting
- âœ… Already have: Can reconstruct distribution from coefficients
- ðŸ”„ Future: Store Metalog in database (not just in-memory)

### **Architectural Considerations for Future Features**

#### **No Streaming/Pagination Needed**
**Why?**
- âœ… Tree depth: Typically 3-5 levels (fits in memory)
- âœ… Tree size: Hundreds of nodes, not millions
- âœ… Metalog storage: ~100 bytes per node (compact)
- âœ… Frontend: Needs full tree for expand/collapse
- âœ… Scenarios: Need full results for comparison (can't compare partial data)

**Handling large result sets:**
- âŒ Raw trial data not returned (10K doubles = 80KB) - Metalog representation used instead
- âŒ Tree node pagination not required - full tree loaded on demand
- âœ… Future consideration: WebSocket for real-time progress updates
- âœ… Future consideration: Caching computed LECs via memoization

#### **Caching Strategy** âœ… (Implemented)
```scala
// TreeCacheManager: per-tree cache lifecycle
trait TreeCacheManager:
  def cacheFor(treeId: TreeId): UIO[RiskResultCache]
  def onTreeStructureChanged(treeId: TreeId): UIO[Unit]
  def deleteTree(treeId: TreeId): UIO[Unit]

// RiskResultCache: per-node result cache
trait RiskResultCache:
  def get(nodeId: NodeId): UIO[Option[RiskResult]]
  def put(nodeId: NodeId, result: RiskResult): UIO[Unit]

// Cache key: NodeId (ULID)
// Invalidate: onTreeStructureChanged clears all nodes for that tree
```

#### **Incremental Recomputation (Future)**
```
When user modifies a single node:
1. Identify affected subtree (node + ancestors)
2. Recompute only that branch (O(depth))
3. Aggregate up to root (O(log n))
4. Don't recompute sibling branches (cached)

Performance:
- Current: Full tree recomputation (O(n))
- Future: Incremental (O(log n))
- Benefit: 10x faster for small changes
```

---

## Deployment Architecture

### **Kubernetes Production Setup** (Intended)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ingress Controller (nginx/traefik)              â”‚
â”‚  - TLS termination                               â”‚
â”‚  - Rate limiting (100 req/sec per IP)           â”‚
â”‚  - Path routing                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service Mesh (Istio/Linkerd)                    â”‚
â”‚  - mTLS between services                         â”‚
â”‚  - JWT validation (Keycloak)                    â”‚
â”‚  - Circuit breakers                              â”‚
â”‚  - Telemetry collection                         â”‚
â”‚  - Authorization (OPA policies)                  â”‚
â”‚  - Injects headers: X-User-Id, X-User-Roles     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Risk Register Service (Deployment)               â”‚
â”‚  - Replicas: 3 (horizontal scaling)             â”‚
â”‚  - Resources: 1 CPU, 2Gi memory per pod         â”‚
â”‚  - Health checks: /health endpoint              â”‚
â”‚  - Stateless (can scale horizontally)           â”‚
â”‚  - Trusts service mesh for auth                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL       â”‚    â”‚ Redis               â”‚
â”‚  - Risk trees    â”‚    â”‚  - Rate limiting    â”‚
â”‚  - Scenarios     â”‚    â”‚  - LEC cache        â”‚
â”‚  - Audit logs    â”‚    â”‚  - Session state    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Component Responsibilities**

| Component | Responsibility | Status |
|-----------|---------------|--------|
| **Ingress** | TLS, rate limiting, routing | Infrastructure |
| **Service Mesh** | Auth, authz, mTLS, telemetry | Infrastructure |
| **Keycloak** | Identity provider (OAuth2/OIDC) | Infrastructure |
| **OPA** | Policy decisions (RBAC) | Infrastructure |
| **Risk Register** | Business logic, simulation | **This Application** |
| **PostgreSQL** | Risk tree persistence | Future |
| **Redis** | Caching, rate limiting | Future |

### **What Risk Register Does NOT Do**
- âŒ JWT parsing/validation (service mesh does this)
- âŒ Password management (Keycloak does this)
- âŒ TLS termination (ingress does this)
- âŒ Policy enforcement (OPA does this)

### **What Risk Register DOES Do**
- âœ… Extract user context from headers (`X-User-Id`, `X-User-Roles`)
- âœ… Propagate context through ZIO fiber refs
- âœ… Audit logging (who did what, when)
- âœ… Business logic (risk simulation)
- âœ… Horizontal scaling (stateless design)

---

## Implementation Status

**For detailed phase tracking, task lists, and implementation plans, see `IMPLEMENTATION_PLAN.md`.**

Key completed work:
- âœ… Configuration management (ZIO Config with application.conf)
- âœ… DTO/Domain separation (validation-during-parsing)
- âœ… Error handling (typed error codes, field paths)
- âœ… Structured logging (ZIO + Logback + OpenTelemetry; JSON encoder swap is a deployment task)

---

## Appendix A: HDR Histogram for Million-Scale Trials

**Date:** January 8, 2026  
**Decision:** High Dynamic Range (HDR) Histogram approach evaluated and determined to be **Not Applicable**  
**Status:** âœ… Sparse storage validated as optimal for this use case

> **Note:** This appendix discusses **HDR Histogram** (a data structure for recording value distributions), not to be confused with **HDR PRNG** (the deterministic pseudo-random number generator used elsewhere in this codebase for reproducible sampling).

### What is HDR Histogram?

High Dynamic Range (HDR) Histogram is a space-efficient data structure for recording and analyzing value distributions. It uses logarithmic bucketing to compress millions of observations into a fixed-size histogram (~10-50KB) while maintaining configurable precision.

**Typical benefits:**
- Constant memory footprint regardless of sample count
- O(1) percentile queries
- Histogram merging for distributed aggregation
- Efficient serialization for persistence

### Why HDR Histogram Was Considered

For large-scale Monte Carlo simulations (1M+ trials), memory consumption of sparse storage could become a concern:
- Current sparse map: 1M trials Ã— 50% probability Ã— 12 bytes/entry = ~6MB per risk
- HDR histogram: Fixed ~10-50KB regardless of trial count

This appears to offer a 100x memory reduction for high-volume simulations.

### Why HDR Histogram Is Not Used

**Critical incompatibility with trial-level aggregation:**

The current architecture requires exact trial-by-trial summation for portfolio aggregation:

```scala
// Required aggregation semantics:
val allTrialIds = Set(0, 1, 2, ..., 9999)
allTrialIds.map { trialId =>
  trialId -> (
    riskA.outcomeOf(trialId) + 
    riskB.outcomeOf(trialId) + 
    riskC.outcomeOf(trialId)
  )
}
```

HDR histograms cannot provide `outcomeOf(trialId)` - they only know "bucket 5000-6000 had 47 occurrences". This breaks:

1. **Exact aggregation:** Cannot sum losses for specific trial IDs across risks
2. **Identity-based composition:** `Identity[RiskResult].combine` requires trial-level granularity
3. **Hierarchical merge:** Bottom-up tree aggregation depends on matching trial sequences
4. **Provenance/reproducibility:** Cannot reconstruct exact trial outcomes from bucketed histogram

### Technical Comparison

| Aspect | Sparse Storage (Current) | HDR Histogram (Rejected) |
|--------|-------------------------|-------------------------|
| **Memory (10k trials, p=0.01)** | ~1.6KB (100 entries) | ~10KB (fixed overhead) |
| **Memory (1M trials, p=0.5)** | ~6MB (500k entries) | ~50KB (compressed) |
| **Trial-level access** | âœ… O(1) lookup by trialId | âŒ Not supported |
| **Exact aggregation** | âœ… Trial-wise summation | âŒ Approximate merge |
| **Parallel scaling** | âœ… Lock-free pure functions | âš ï¸ Synchronized updates |
| **Distributed compute** | âœ… Partition trials across workers | âŒ Loses trial alignment |
| **Provenance** | âœ… Full trial reconstruction | âŒ Lossy compression |
| **Percentile queries** | âœ… O(log n) via lazy TreeMap | âœ… O(1) from histogram |

### Alternative Scaling Path: Horizontal Distribution

Instead of algorithmic compression (HDR), the current architecture scales horizontally through referential transparency:

```scala
// Each worker computes disjoint trial range
workers.map { case (worker, trialRange) =>
  worker.execute {
    val sampler = RiskSampler.fromDistribution(...)
    trialRange.map(t => t -> sampler.sampleLoss(t)).toMap
  }
}.reduce { (mapA, mapB) =>
  // Merge disjoint maps - O(n) but no trial overlap
  mapA ++ mapB
}
```

**Benefits of horizontal scaling:**
- Pure functions enable trivial distribution (no coordination)
- Deterministic PRNG allows any trial partitioning strategy  
- Associative merge preserves exact aggregation semantics
- Works with current codebase (zero architectural changes)

### Conclusion

Sparse storage using `Map[TrialId, Loss]` is the optimal approach because:

1. **Sparse efficiency:** Already memory-efficient for typical low-probability events
2. **Exact semantics:** Preserves mathematical correctness of portfolio aggregation
3. **Horizontal scaling:** Achieves 100M+ trial scale through distribution, not approximation
4. **Provenance intact:** Maintains full reproducibility guarantees
5. **Architecture simplicity:** No need for storage abstraction layer or mode switching

HDR Histogram would require fundamental architecture changes (approximate merge, loss of provenance) for questionable memory savings at scales the application is unlikely to encounter.

**Decision:** Mark HDR Histogram as "Evaluated - Not Applicable" and continue with sparse storage approach.

---

### **Key Architectural Decisions**

#### **ADR-001: Iron Refinement Types for Domain Models**
- **Decision:** Use Iron opaque types for all validated domain primitives
- **Rationale:** Compile-time safety, zero runtime overhead, self-documenting
- **Status:** âœ… Implemented
- **Alternatives Rejected:** Runtime validation only, custom wrapper types

#### **ADR-002: ZIO Prelude for Error Accumulation**
- **Decision:** Use `Validation[String, A]` for parallel error collection
- **Rationale:** Better UX (all errors at once), functional, composable
- **Status:** âœ… Implemented
- **Alternatives Rejected:** Fail-fast Either, exceptions

#### **ADR-003: Separate Config from Computation**
- **Decision:** POST creates config, GET /lec computes results
- **Rationale:** Config is fast/synchronous, computation is slow/expensive
- **Status:** âœ… Implemented
- **Alternatives Rejected:** POST runs simulation immediately

#### **ADR-004: Metalog Storage (No Raw Trials)**
- **Decision:** Store Metalog coefficients, not raw trial data
- **Rationale:** 1000x size reduction, reproducible, fast serialization
- **Status:** âœ… Implemented
- **Alternatives Rejected:** Store raw trials, store histograms

#### **ADR-005: SSE for Serverâ†’Client Push**
- **Decision:** Use Server-Sent Events for cache invalidation notifications
- **Rationale:** Simple unidirectional streaming, browser-native support
- **Status:** âœ… Implemented

#### **ADR-009: Simulation Configuration**
- **Decision:** Simulation parameters from server config, not API parameters
- **Rationale:** Consistent results, simpler API
- **Status:** âœ… Implemented

#### **ADR-010: Flat Node Storage**
- **Decision:** Flat node collection with ID references, not recursive nesting
- **Rationale:** Enables O(1) lookup via TreeIndex, simplifies serialization
- **Status:** âœ… Implemented

#### **ADR-014: LEC Caching Architecture**
- **Decision:** Per-tree cache with O(depth) node-to-root invalidation
- **Rationale:** Incremental recomputation, memory isolation per tree
- **Status:** âœ… Implemented

#### **ADR-015: Cache-Aside LEC Resolution**
- **Decision:** RiskResultResolver uses cache-aside pattern for LEC queries
- **Rationale:** Lazy computation, transparent caching, simple invalidation
- **Status:** âœ… Accepted

#### **ADR-018: Nominal Wrappers for Identity Types**
- **Decision:** `NodeId` and `TreeId` case class wrappers over `SafeId`
- **Rationale:** Prevents accidental interchange of semantically distinct IDs
- **Status:** âœ… Implemented

---

## Appendix B: ZIO Metrics Bridge for Runtime Observability

**Status:** Available but not yet implemented  
**Priority:** Low (nice-to-have)

### Context

The current telemetry implementation uses OpenTelemetry directly for:
- **Tracing** - Distributed spans via `Tracing` service
- **Metrics** - Business metrics via `Meter` service (counters, histograms)

ZIO has built-in runtime metrics (fiber counts, execution times, etc.) that are separate from our OpenTelemetry instrumentation.

### Technical Option

zio-telemetry provides `OpenTelemetry.zioMetrics` layer that bridges ZIO's internal metrics to OpenTelemetry exporters. Combined with `DefaultJvmMetrics`, this provides comprehensive runtime observability with zero additional instrumentation code.

### Implementation

```scala
// In Application.scala appLayer, add:
import zio.metrics.jvm.DefaultJvmMetrics

val appLayer: ZLayer[Any, Throwable, ...] =
  ZLayer.make[...](
    // ... existing layers ...
    
    // Optional: Bridge ZIO runtime metrics to OpenTelemetry
    OpenTelemetry.zioMetrics,
    
    // Optional: JVM metrics (heap, GC, threads)
    DefaultJvmMetrics.liveV2.unit
  )
```

### Metrics Provided

| Category | Metrics |
|----------|---------|
| **JVM Heap** | `jvm.memory.used`, `jvm.memory.committed`, `jvm.memory.max` |
| **GC** | `jvm.gc.duration`, `jvm.gc.count` |
| **Threads** | `jvm.threads.count`, `jvm.threads.daemon` |
| **ZIO Runtime** | Fiber execution times, fiber counts (when enabled) |

### Trade-offs

| Pros | Cons |
|------|------|
| Zero instrumentation code | Additional telemetry volume |
| Standard JVM metrics | May clutter dashboards |
| Useful for capacity planning | Requires filtering in observability backend |
| Free with existing OpenTelemetry setup | Minor runtime overhead |

### Recommendation

**Defer until production deployment.** JVM metrics are most valuable when:
1. Running in containers with resource limits
2. Debugging memory leaks or GC pressure
3. Capacity planning for scaling decisions

For development, the current business metrics (operations counter, simulation duration, trials counter) provide sufficient observability.

---

**Document Status:** Architecture reference (stable)
**Last Updated:** February 9, 2026
**Implementation Tracking:** See IMPLEMENTATION_PLAN.md
